# Capstone
Capstone for my Master's Degree

Summary: The goal of this project was to predict the number of readers of a book using its description. It used machine learning and natural language processing algorithms. The results attempted to indicate if particular words or phrases in a book’s description predict how many people will rate a book. The number ratings a book has was used as a proxy for the number of book sales. This project is relevant to the publishing market because a book’s description is part of the buying process that publishers and authors have substantial control over. Knowing what kinds of descriptions result in more people rating, and therefore buying, a book would support future book marketing decisions.

Data: The data was from the most popular site for book readers, GoodReads.com. The project used the datasets goodreads_books_romance and goodreads_books_children from sites.google.com/eng.ucsd.edu/ucsdbookgraph/home, which included average rating, number of ratings, and a description for over 300,000 romance books and over 124,000 children’s books. The features I ended up using were description text, description word count, and book rating count. The preprocessing I did to the dataset is described in the first phase of my project.
Steps: This project had three phases: data preprocessing, text analysis, and model building. 
     
a.	The data preprocessing phase consisted of importing the book data from .json files. Then by exploring the data it became obvious that some of the books should be removed from the dataset before the text analysis phase. Books with language codes other than English, and books with descriptions too short to properly analyze were removed. Then the dataset was reduced to only the book features needed: review count, average rating, description and rating count. Next, after looking at a few sample descriptions, I realized they contained html tags from the original GoodReads website. I used BeautifulSoup to identify and remove the tags. In order to prepare the descriptions for text analysis I needed to remove the punctuation and stop words (“the”, “of”, “and” etc.) and stem and lemmatize the remaining words. At first I used spaCy to do this, but later realized that NLTK could do all those tasks plus more easily tokenize the descriptions into word lists. Then I plotted the distribution of book rating counts. It became obvious that some books had extremely large rating counts that could skew my results. I removed any books with rating counts three or more standard distributions from the mean number of rating counts. The last step in my preprocessing was to group the rating counts into bins. At first I tried ten bins, then five, and finally just two. Later, I added a feature with the word count of the descriptions. 
   
b.	I began to analyze the text using the bag-of-words method to create a corpus of words used in all the book descriptions. This corpus was then used to create a Latent Dirichlet Allocation (LDA) model, which yielded topics from all the documents. In the end I used CountVectorizer() and TfIdfVectorizer() from the scikit-learn library. CountVectorizer() creates a document term matrix, where every row is a book description and every column in a term used in all the book descriptions. The matrix contains 1’s whenever that row’s document contains that column’s term. The TfIdfVectorizer() method takes the document term matrix and converts it into tf-idf representation. Tf-Idf stands for term frequency times inverse document frequency. This method is useful when the documents contain a lot of very frequent terms, as is true in this project. Lastly, I added a column to the tf-idf matrix with the word count of each book description.
    
c.	Five different classifiers were used in this project: logistic regression, multinomial naïve bayes, linear support vector machine, stochastic gradient descent, and a sequential neural network. The first four were given the word count variable but I was unable to configure the neural network to incorporate it. I also attempted to use the random forest classifier but was unable to get it to work. The accuracy of each classifier is listed in the results table below. The most successful classifier was logistic regression.
    
Evaulation Metrics: The holdout method was used to create training and test sets. The test set was 30% of the data. This project used six evaluation metrics to determine how accurate the models were at predicting the number of book ratings: confusion matrix, accuracy percentage, precision, recall, f1-score, and ROC area-under-curve score. I also tried to use log-loss, but got the same result for all five classifiers, invalidating it as an evaluation metric. Below are the results of all five classifiers, as well as a table for easy comparison. After seeing the results for the romance genre books, I also ran the classifiers on the children’s genre books to see if the classifiers were more accurate. I thought the descriptions for children’s books might be simpler and therefore easier to classify.

Conclusion: In conclusion this data was difficult to classify using machine learning algorithms. This could be for many reasons. First the data only had two parameters, description and word count, to predict the target, number of ratings. Adding more parameters, like title or series name, might improve the accuracy. Second, the decision I tried to predict, how many people read the description and decided to read the book and rate it on GoodReads, is not a simple decision. The customer’s choice is influenced by many factors not included in this project, such as price or available platforms (paper vs digital etc.). The choice is also not made quickly and is therefore harder to predict. In order to more accurately predict if customers choose to buy a book based on its description, I would need more direct data. The actual sales data of these books would be ideal.
